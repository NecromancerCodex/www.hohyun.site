import math
import torch
from app.diffusers.services.diffusion.pipeline_manager import get_pipeline, get_refiner_pipeline
from app.diffusers.core.config import (
    DEFAULT_WIDTH, DEFAULT_HEIGHT, DEFAULT_STEPS, DEFAULT_GUIDANCE,
    MAX_WIDTH, MAX_HEIGHT, MAX_STEPS, DEVICE, USE_REFINER, DEFAULT_REFINER_STRENGTH
)

def _clamp_int(v: int, lo: int, hi: int) -> int:
    """ê°’ì„ ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ë¡œ ì œí•œ"""
    return max(lo, min(hi, int(v)))

def _round_to_multiple(v: int, base: int = 8) -> int:
    """ê°’ì„ baseì˜ ë°°ìˆ˜ë¡œ ë‚´ë¦¼"""
    return int(math.floor(v / base) * base)

def generate_txt2img(
    prompt: str,
    negative_prompt: str | None = None,
    width: int = DEFAULT_WIDTH,
    height: int = DEFAULT_HEIGHT,
    steps: int = DEFAULT_STEPS,
    guidance_scale: float = DEFAULT_GUIDANCE,
    seed: int | None = None,
    use_refiner: bool | None = None,
    refiner_strength: float | None = None,
):
    """
    í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„±
    
    Args:
        prompt: ìƒì„±í•  ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸
        negative_prompt: ì œì™¸í•  ìš”ì†Œë¥¼ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸
        width: ì´ë¯¸ì§€ ë„ˆë¹„ (8ì˜ ë°°ìˆ˜ë¡œ ìë™ ì¡°ì •, MAX_WIDTHë¡œ ì œí•œ)
        height: ì´ë¯¸ì§€ ë†’ì´ (8ì˜ ë°°ìˆ˜ë¡œ ìë™ ì¡°ì •, MAX_HEIGHTë¡œ ì œí•œ)
        steps: ìƒ˜í”Œë§ ìŠ¤í… ìˆ˜ (MAX_STEPSë¡œ ì œí•œ)
        guidance_scale: CFG Scale (ì°½ì˜ì„± ì¡°ì ˆ, SDXL ê¶Œì¥ 5-9)
        seed: ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)
    
    Returns:
        (PIL.Image, dict): ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ë©”íƒ€ë°ì´í„°
    """
    pipe = get_pipeline()

    # íŒŒë¼ë¯¸í„° ë³´ì • (OOM ë°©ì§€)
    # 8GB VRAMì—ì„œëŠ” 1024x1024ê°€ ì•ˆì „í•œ ìµœëŒ€ í•´ìƒë„
    width = _round_to_multiple(_clamp_int(width, 64, MAX_WIDTH), 8)
    height = _round_to_multiple(_clamp_int(height, 64, MAX_HEIGHT), 8)
    steps = _clamp_int(steps, 1, MAX_STEPS)
    
    # í•´ìƒë„ ê²½ê³ 
    if width > 1024 or height > 1024:
        import warnings
        warnings.warn(
            f"âš ï¸  í•´ìƒë„ {width}x{height}ì€ 8GB VRAMì—ì„œ OOM ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. "
            f"1024x1024 ì´í•˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
            UserWarning
        )

    # ì‹œë“œ ì„¤ì •
    gen = None
    if seed is not None:
        device = "cuda" if (DEVICE == "cuda" and torch.cuda.is_available()) else "cpu"
        gen = torch.Generator(device=device).manual_seed(int(seed))

    # SDXL ì´ë¯¸ì§€ ìƒì„±
    # guidance_scale: 5~9 ê¶Œì¥ (ê¸°ë³¸ 7.0)
    # ë„ˆë¬´ ë†’ìœ¼ë©´ ê³¼í¬í™”, ë„ˆë¬´ ë‚®ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ë¬´ì‹œ
    
    # Refiner ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_refiner_flag = use_refiner if use_refiner is not None else USE_REFINER
    refiner = get_refiner_pipeline() if use_refiner_flag else None
    
    # upcast_vae deprecation ê²½ê³  ë°©ì§€
    import warnings
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning, message=".*upcast_vae.*")
        
        if refiner is not None:
            # Refiner ì‚¬ìš© ì‹œ: Base ëª¨ë¸ì—ì„œ Latent ì¶œë ¥ ë°›ê¸°
            print("ğŸ”„ Base ëª¨ë¸ë¡œ Latent ìƒì„± ì¤‘...")
            result = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=float(guidance_scale),
                generator=gen,
                output_type="latent",  # Latent space ì¶œë ¥ (VAE ë””ì½”ë”© ì „)
            )
            
            # Base ëª¨ë¸ì˜ Latent ì¶œë ¥
            latents = result.images[0]  # torch.Tensor (latent space)
            
            # Refinerë¡œ Latent ê°œì„  (ì´ë¯¸ì§€ êµ¬ì¡°ì™€ ë™ì¼)
            print("âœ¨ Refinerë¡œ Latent ê°œì„  ì¤‘... (ë””í…Œì¼ í–¥ìƒ)")
            strength = refiner_strength if refiner_strength is not None else DEFAULT_REFINER_STRENGTH
            strength = max(0.0, min(1.0, float(strength)))  # 0.0~1.0 ë²”ìœ„ë¡œ ì œí•œ
            
            refiner_gen = None
            if seed is not None:
                device = "cuda" if (DEVICE == "cuda" and torch.cuda.is_available()) else "cpu"
                refiner_gen = torch.Generator(device=device).manual_seed(int(seed) + 1)  # ì‹œë“œ ë³€ê²½
            
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=FutureWarning, message=".*upcast_vae.*")
                
                # Refinerê°€ Latentë¥¼ ë°›ì•„ì„œ ê°œì„  (ì´ë¯¸ì§€ êµ¬ì¡°ì™€ ì¼ì¹˜)
                refined_result = refiner(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=latents,  # Base ëª¨ë¸ì˜ Latent ì¶œë ¥ ì§ì ‘ ì „ë‹¬
                    strength=strength,  # 0.25~0.3 ê¶Œì¥ (ë””í…Œì¼ ì‚´ë¦¬ë©´ì„œ ì›ë³¸ ìœ ì§€)
                    num_inference_steps=max(1, int(steps * 0.5)),  # RefinerëŠ” ì ˆë°˜ ìŠ¤í… ì‚¬ìš©
                    guidance_scale=float(guidance_scale),
                    generator=refiner_gen,
                    output_type="pil",  # ìµœì¢… ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©
                )
            
            image = refined_result.images[0]
            print(f"âœ… Refiner ì ìš© ì™„ë£Œ (strength: {strength})")
            
            # Refiner ë©”ëª¨ë¦¬ ì •ë¦¬
            if DEVICE == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                import gc
                gc.collect()
        else:
            # Refiner ë¯¸ì‚¬ìš© ì‹œ: Base ëª¨ë¸ì—ì„œ ì§ì ‘ ì´ë¯¸ì§€ ìƒì„±
            result = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=float(guidance_scale),
                generator=gen,
                output_type="pil",  # PIL Imageë¡œ ëª…ì‹œì  ë°˜í™˜
            )
            image = result.images[0]

    # ë©”íƒ€ë°ì´í„° ìƒì„±
    meta = {
        "model_id": getattr(pipe, "name_or_path", None),
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "width": width,
        "height": height,
        "steps": steps,
        "guidance_scale": float(guidance_scale),
        "seed": seed,
        "device": "cuda" if (DEVICE == "cuda" and torch.cuda.is_available()) else "cpu",
        "refiner_used": (use_refiner if use_refiner is not None else USE_REFINER) and get_refiner_pipeline() is not None,
        "refiner_strength": refiner_strength if (use_refiner if use_refiner is not None else USE_REFINER) else None,
    }
    return image, meta
