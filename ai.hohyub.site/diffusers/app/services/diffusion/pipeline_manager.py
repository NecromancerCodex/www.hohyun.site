import torch
from diffusers import (
    AutoPipelineForText2Image,
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
    UNet2DConditionModel,
    AutoencoderKL,
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler,
)
from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer
from safetensors import safe_open
from app.diffusers.core.config import MODEL_ID, DEVICE, DTYPE, HF_CACHE_DIR, SCHEDULER_TYPE, USE_KARRAS, USE_REFINER, DEFAULT_REFINER_STRENGTH

_PIPELINE = None
_PIPELINE_LOADED = False  # íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì—¬ë¶€ ì¶”ì 
_CURRENT_MODEL_ID = None  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ID
_REFINER_PIPELINE = None
_REFINER_LOADED = False  # Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì—¬ë¶€ ì¶”ì 

def _torch_dtype():
    """ì„¤ì •ì— ë”°ë¥¸ torch dtype ë°˜í™˜"""
    if DTYPE.lower() == "float16":
        return torch.float16
    if DTYPE.lower() == "bfloat16":
        return torch.bfloat16
    return torch.float32

def _load_from_single_files(model_dir):
    """
    ë‹¨ì¼ safetensors íŒŒì¼ì—ì„œ SDXL íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    - sd_xl_base_1.0.safetensors: UNet
    - sdxl.vae.safetensors: VAE
    - text_encoder, text_encoder_2: ë¡œì»¬ ëª¨ë¸ì—ì„œ ë¡œë“œ
    """
    from pathlib import Path
    model_path = Path(model_dir)
    dtype = _torch_dtype()
    
    print("ğŸ“¦ ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ ì¤‘...")
    
    # 1. Text Encoders (ë¡œì»¬ ëª¨ë¸ì—ì„œ ë¡œë“œ)
    print("  [1/4] Text Encoders ë¡œë“œ ì¤‘ (ë¡œì»¬)...")
    if (model_path / "text_encoder").exists() and (model_path / "text_encoder_2").exists():
        # ë¡œì»¬ì— í‘œì¤€ í˜•ì‹ì´ ìˆìœ¼ë©´ ë¡œì»¬ ì‚¬ìš©
        text_encoder = CLIPTextModel.from_pretrained(
            str(model_path),
            subfolder="text_encoder",
            torch_dtype=dtype,
            local_files_only=True,
        )
        text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(
            str(model_path),
            subfolder="text_encoder_2",
            torch_dtype=dtype,
            local_files_only=True,
        )
        tokenizer = CLIPTokenizer.from_pretrained(
            str(model_path),
            subfolder="tokenizer",
            local_files_only=True,
        )
        tokenizer_2 = CLIPTokenizer.from_pretrained(
            str(model_path),
            subfolder="tokenizer_2",
            local_files_only=True,
        )
        print("  âœ… Text Encoders ë¡œë“œ ì™„ë£Œ (ë¡œì»¬)")
    else:
        raise FileNotFoundError(
            f"ë¡œì»¬ Text Encodersë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}/text_encoder, {model_path}/text_encoder_2\n"
            "í‘œì¤€ diffusers í˜•ì‹ì˜ ë¡œì»¬ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )
    
    # 2. UNet (ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ë¡œë“œ)
    print("  [2/4] UNet ë¡œë“œ ì¤‘...")
    unet_path = model_path / "sd_xl_base_1.0.safetensors"
    if not unet_path.exists():
        raise FileNotFoundError(f"UNet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {unet_path}")
    
    # UNet configëŠ” ë¡œì»¬ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if (model_path / "unet").exists():
        unet = UNet2DConditionModel.from_pretrained(
            str(model_path),
            subfolder="unet",
            torch_dtype=dtype,
            local_files_only=True,
        )
    else:
        raise FileNotFoundError(
            f"ë¡œì»¬ UNetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}/unet\n"
            "í‘œì¤€ diffusers í˜•ì‹ì˜ ë¡œì»¬ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )
    
    # ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # sd_xl_base_1.0.safetensorsëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ
    # UNet ëª¨ë¸ì˜ í‚¤ êµ¬ì¡° í™•ì¸
    unet_model_keys = set(unet.state_dict().keys())
    
    # ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë“  í‚¤ ë¡œë“œ
    file_state_dict = {}
    with safe_open(str(unet_path), framework="pt", device="cpu") as f:
        for key in f.keys():
            file_state_dict[key] = f.get_tensor(key)
    
    # UNet ëª¨ë¸ í‚¤ì™€ ë§¤ì¹­
    unet_state_dict = {}
    matched_count = 0
    
    # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
    for model_key in unet_model_keys:
        if model_key in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[model_key]
            matched_count += 1
        # 2. ComfyUI í˜•ì‹ ë³€í™˜ ì‹œë„ (model.diffusion_model. -> )
        elif f"model.diffusion_model.{model_key}" in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[f"model.diffusion_model.{model_key}"]
            matched_count += 1
        # 3. diffusion_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"diffusion_model.{model_key}" in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[f"diffusion_model.{model_key}"]
            matched_count += 1
    
    print(f"  ğŸ“Š í‚¤ ë§¤ì¹­: {matched_count}/{len(unet_model_keys)}ê°œ")
    
    # ë¡œë“œëœ í‚¤ í™•ì¸
    missing_keys, unexpected_keys = unet.load_state_dict(unet_state_dict, strict=False)
    if len(missing_keys) > 100:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
        print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
    elif missing_keys:
        print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
    if unexpected_keys:
        print(f"  âš ï¸  UNet ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
    print(f"  âœ… UNet ë¡œë“œ ì™„ë£Œ")
    
    # 3. VAE (ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ë¡œë“œ)
    print("  [3/4] VAE ë¡œë“œ ì¤‘...")
    vae_path = model_path / "sdxl.vae.safetensors"
    if not vae_path.exists():
        raise FileNotFoundError(f"VAE íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vae_path}")
    
    # VAEëŠ” ë¡œì»¬ ëª¨ë¸ì—ì„œ ë¡œë“œ
    if (model_path / "vae").exists():
        # í‘œì¤€ diffusers í˜•ì‹ VAE ì‚¬ìš©
        vae = AutoencoderKL.from_pretrained(
            str(model_path),
            subfolder="vae",
            torch_dtype=dtype,
            local_files_only=True,
        )
    else:
        # ë‹¨ì¼ íŒŒì¼ í˜•ì‹ VAE ì‚¬ìš©
        vae_config_path = model_path / "config.json"
        if vae_config_path.exists():
            # ë¡œì»¬ config ì‚¬ìš©
            import json
            vae_config = json.loads(vae_config_path.read_text())
            # _class_name, _diffusers_version ë“± ì œê±°
            vae_config_clean = {k: v for k, v in vae_config.items() if not k.startswith('_')}
            vae = AutoencoderKL(**vae_config_clean)
        else:
            raise FileNotFoundError(
                f"ë¡œì»¬ VAEë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}/vae ë˜ëŠ” {model_path}/config.json\n"
                "í‘œì¤€ diffusers í˜•ì‹ì˜ ë¡œì»¬ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
    
    # ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # VAE ëª¨ë¸ì˜ í‚¤ êµ¬ì¡° í™•ì¸
    vae_model_keys = set(vae.state_dict().keys())
    
    # ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë“  í‚¤ ë¡œë“œ
    file_state_dict = {}
    with safe_open(str(vae_path), framework="pt", device="cpu") as f:
        for key in f.keys():
            file_state_dict[key] = f.get_tensor(key)
    
    # VAE ëª¨ë¸ í‚¤ì™€ ë§¤ì¹­
    vae_state_dict = {}
    matched_count = 0
    
    # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
    for model_key in vae_model_keys:
        if model_key in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[model_key]
            matched_count += 1
        # 2. first_stage_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"first_stage_model.{model_key}" in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[f"first_stage_model.{model_key}"]
            matched_count += 1
        # 3. model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"model.{model_key}" in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[f"model.{model_key}"]
            matched_count += 1
    
    print(f"  ğŸ“Š í‚¤ ë§¤ì¹­: {matched_count}/{len(vae_model_keys)}ê°œ")
    
    # ë¡œë“œëœ í‚¤ í™•ì¸
    missing_keys, unexpected_keys = vae.load_state_dict(vae_state_dict, strict=False)
    if len(missing_keys) > 50:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"  âš ï¸  VAE ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
        print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
    elif missing_keys:
        print(f"  âš ï¸  VAE ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
    if unexpected_keys:
        print(f"  âš ï¸  VAE ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
    # VAE dtype ì„¤ì • (ê²½ê³  ë°©ì§€)
    # VAEëŠ” ë””ì½”ë”© ì‹œ float32ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    vae = vae.to(dtype=dtype)
    # upcast_vae deprecation ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
    if hasattr(vae, 'enable_slicing'):
        vae.enable_slicing()
    if hasattr(vae, 'enable_tiling'):
        vae.enable_tiling()
    print(f"  âœ… VAE ë¡œë“œ ì™„ë£Œ")
    
    # 4. Scheduler (ë¡œì»¬ ëª¨ë¸ì—ì„œ ë¡œë“œ)
    print("  [4/4] Scheduler ë¡œë“œ ì¤‘ (ë¡œì»¬)...")
    if (model_path / "scheduler").exists():
        if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
            # DPM++ 2M Karras (ê³ í’ˆì§ˆ ì¡°í•©)
            scheduler = DPMSolverMultistepScheduler.from_pretrained(
                str(model_path),
                subfolder="scheduler",
                local_files_only=True,
            )
            # Karras ì‹œê·¸ë§ˆ ìŠ¤ì¼€ì¤„ ì ìš©
            scheduler = DPMSolverMultistepScheduler.from_config(
                scheduler.config,
                use_karras=True,
            )
            print("  âœ… DPM++ Multistep Scheduler (Karras) ë¡œë“œ ì™„ë£Œ (ë¡œì»¬)")
        else:
            # Euler (ê¸°ë³¸)
            scheduler = EulerDiscreteScheduler.from_pretrained(
                str(model_path),
                subfolder="scheduler",
                local_files_only=True,
            )
            print("  âœ… Euler Discrete Scheduler ë¡œë“œ ì™„ë£Œ (ë¡œì»¬)")
    else:
        raise FileNotFoundError(
            f"ë¡œì»¬ Schedulerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}/scheduler\n"
            "í‘œì¤€ diffusers í˜•ì‹ì˜ ë¡œì»¬ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )
    
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    print("ğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
    pipe = StableDiffusionXLPipeline(
        vae=vae,
        text_encoder=text_encoder,
        text_encoder_2=text_encoder_2,
        tokenizer=tokenizer,
        tokenizer_2=tokenizer_2,
        unet=unet,
        scheduler=scheduler,
    )
    
    return pipe

def get_pipeline():
    """
    SDXL íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤ ë¡œë“œ ë° ìµœì í™”
    RTX 4060 8GB í™˜ê²½ì— ìµœì í™”ë¨
    ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ ì§€ì›
    """
    global _PIPELINE
    if _PIPELINE is not None:
        return _PIPELINE

    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_ID}")
    dtype = _torch_dtype()

    # ë¡œì»¬ ëª¨ë¸ì¸ì§€ Hugging Face ëª¨ë¸ì¸ì§€ í™•ì¸
    from pathlib import Path
    model_path = Path(MODEL_ID)
    is_local = model_path.exists() and (model_path / "model_index.json").exists()
    
    if is_local:
        print(f"ğŸ“ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {MODEL_ID}")
        
        # í‘œì¤€ diffusers í˜•ì‹ í™•ì¸ (ìš°ì„ )
        has_text_encoder = (model_path / "text_encoder").exists()
        has_unet = (model_path / "unet").exists()
        has_vae = (model_path / "vae").exists()
        
        # ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ í™•ì¸ (ëŒ€ì²´)
        has_unet_file = (model_path / "sd_xl_base_1.0.safetensors").exists()
        has_vae_file = (model_path / "sdxl.vae.safetensors").exists()
        
        if has_text_encoder and has_unet and has_vae:
            # í‘œì¤€ diffusers í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (ì™„ì „ ë¡œì»¬)
            print("  ğŸ“¦ í‘œì¤€ diffusers í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (ì™„ì „ ë¡œì»¬)")
            try:
                # VAE: sdxl.vae.safetensors ìš°ì„  ì‚¬ìš© (ìƒ‰ê° ë³´ì¡´)
                # ë‹¨ì¼ íŒŒì¼ í˜•ì‹ì˜ VAEê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                vae_single_file = model_path / "sdxl.vae.safetensors"
                if vae_single_file.exists():
                    print("  ğŸ¨ sdxl.vae.safetensors ì‚¬ìš© (ìƒ‰ê° ë³´ì¡´)")
                    # VAEë¥¼ ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¡œë“œ
                    from diffusers import AutoencoderKL
                    vae = AutoencoderKL.from_pretrained(
                        MODEL_ID,
                        subfolder="vae",
                        torch_dtype=dtype,
                        local_files_only=True,
                    )
                    # sdxl.vae.safetensorsì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
                    from safetensors import safe_open
                    vae_state_dict = {}
                    with safe_open(str(vae_single_file), framework="pt", device="cpu") as f:
                        for key in f.keys():
                            if key.startswith("first_stage_model."):
                                vae_state_dict[key.replace("first_stage_model.", "")] = f.get_tensor(key)
                            elif key.startswith("vae."):
                                vae_state_dict[key[4:]] = f.get_tensor(key)
                            elif key.startswith("model."):
                                vae_state_dict[key[6:]] = f.get_tensor(key)
                            else:
                                vae_state_dict[key] = f.get_tensor(key)
                    vae.load_state_dict(vae_state_dict, strict=False)
                    vae = vae.to(dtype=dtype)
                    
                    # ë‚˜ë¨¸ì§€ ì»´í¬ë„ŒíŠ¸ëŠ” í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ
                    pipe = AutoPipelineForText2Image.from_pretrained(
                        MODEL_ID,
                        torch_dtype=dtype,
                        variant=None,
                        use_safetensors=True,
                        local_files_only=True,
                    )
                    # VAE êµì²´
                    pipe.vae = vae
                else:
                    # í‘œì¤€ í˜•ì‹ VAE ì‚¬ìš©
                    pipe = AutoPipelineForText2Image.from_pretrained(
                        MODEL_ID,
                        torch_dtype=dtype,  # dtype ë³€ìˆ˜ ì‚¬ìš© (float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
                        variant=None,
                        use_safetensors=True,
                        local_files_only=True,  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                    )
                
                # torch_dtypeìœ¼ë¡œ ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”
                # (from_pretrainedì˜ torch_dtype íŒŒë¼ë¯¸í„°ê°€ ì´ë¯¸ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì— ì ìš©ë¨)
                
                # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© (í‘œì¤€ í˜•ì‹)
                if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
                    print("  ğŸ”¥ Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì¤‘...")
                    pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                        pipe.scheduler.config,
                        use_karras=True,
                    )
                    print("  âœ… DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ í‘œì¤€ í˜•ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("  ğŸ’¡ download_model_local.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                raise
        elif has_unet_file and has_vae_file:
            # ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (Text EncodersëŠ” ë¡œì»¬ì—ì„œ)
            print("  ğŸ“¦ ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (Text EncodersëŠ” ë¡œì»¬ì—ì„œ)")
            try:
                pipe = _load_from_single_files(model_path)
            except Exception as e:
                print(f"  âŒ ë‹¨ì¼ íŒŒì¼ í˜•ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                raise
        else:
            raise ValueError(
                "ë¡œì»¬ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í˜•ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤:\n"
                "  1. í‘œì¤€ diffusers í˜•ì‹: text_encoder/, unet/, vae/ í´ë”\n"
                "  2. ë‹¨ì¼ íŒŒì¼ í˜•ì‹: sd_xl_base_1.0.safetensors, sdxl.vae.safetensors\n"
                "download_model_local.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
    else:
        # ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ ë°œìƒ
        raise ValueError(
            f"ë¡œì»¬ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_ID}\n"
            "ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ í™•ì¸: {LOCAL_MODEL_DIR}\n"
            "í‘œì¤€ diffusers í˜•ì‹ì˜ ë¡œì»¬ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )

    # âœ… RTX 4060 8GB ìµœì í™” ì˜µì…˜
    
    # 1. xFormers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ (ê°€ì¥ ì¤‘ìš”!)
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸  xFormers í™œì„±í™” ì‹¤íŒ¨: {e}")
        # xFormers ì‹¤íŒ¨ ì‹œ attention slicing ì‚¬ìš©
        try:
            pipe.enable_attention_slicing(slice_size="auto")
            print("âœ… Attention Slicing í™œì„±í™” (xFormers ëŒ€ì²´)")
        except Exception:
            pass

    # 2. VAE Tiling (ê³ í•´ìƒë„/ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì•ˆì •ì„±) - í•„ìˆ˜!
    try:
        pipe.enable_vae_tiling()
        print("âœ… VAE Tiling í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
    except Exception as e:
        print(f"âš ï¸  VAE Tiling í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    # 3. VAE Slicing (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)
    try:
        pipe.enable_vae_slicing()
        print("âœ… VAE Slicing í™œì„±í™” (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)")
    except Exception as e:
        print(f"âš ï¸  VAE Slicing í™œì„±í™” ì‹¤íŒ¨: {e}")

    # ë””ë°”ì´ìŠ¤ ì´ë™
    if DEVICE == "cuda" and torch.cuda.is_available():
        pipe = pipe.to("cuda")
        print(f"âœ… CUDA ë””ë°”ì´ìŠ¤ë¡œ ì´ë™: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        pipe = pipe.to("cpu")
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ (ëŠë¦¼)")

    _PIPELINE = pipe
    _PIPELINE_LOADED = True # íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ
    _CURRENT_MODEL_ID = MODEL_ID
    print("âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
    return _PIPELINE

def switch_model(model_id: str):
    """
    ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜ (ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ í¬í•¨)
    
    Args:
        model_id: ëª¨ë¸ ID (ì˜ˆ: "sdxl_base", "cyberrealisticpony_v150", "dwkorean_doll_likeliness_v1")
    """
    global _PIPELINE, _PIPELINE_LOADED, _CURRENT_MODEL_ID
    
    print(f"ğŸ” switch_model í˜¸ì¶œ: model_id={model_id}, í˜„ì¬ ëª¨ë¸={_CURRENT_MODEL_ID}")
    
    # ê°™ì€ ëª¨ë¸ì´ë©´ ì „í™˜ ë¶ˆí•„ìš”
    if _CURRENT_MODEL_ID == model_id and _PIPELINE is not None:
        print(f"â„¹ï¸  ê°™ì€ ëª¨ë¸ì´ë¯€ë¡œ ì „í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_id}")
        return
    
    from app.diffusers.core.models import get_model_info
    from pathlib import Path
    
    model_info = get_model_info(model_id)
    if not model_info:
        raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
    
    print(f"ğŸ”„ ëª¨ë¸ ì „í™˜ ì¤‘: {model_info['name']} ({model_id})")
    
    # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ë©”ëª¨ë¦¬ í•´ì œ
    if _PIPELINE is not None:
        del _PIPELINE
        import gc
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
    
    _PIPELINE = None
    _PIPELINE_LOADED = False
    
    # ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë“œ
    if model_info["type"] == "checkpoint" or model_info["type"] in ["cyber_realistic", "korean_doll"]:
        _load_checkpoint_model(model_info)
    else:
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ë¡œì§)
        get_pipeline()
    
    _CURRENT_MODEL_ID = model_id
    print(f"âœ… ëª¨ë¸ ì „í™˜ ì™„ë£Œ: {model_info['name']}")

def _load_checkpoint_model(model_info: dict):
    """
    ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸(.safetensors) ë¡œë“œ
    
    Args:
        model_info: ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    global _PIPELINE, _PIPELINE_LOADED
    
    from pathlib import Path
    from app.diffusers.core.config import LOCAL_MODEL_DIR
    
    model_dir = Path(LOCAL_MODEL_DIR)
    checkpoint_file = model_dir / model_info["file"]
    
    if not checkpoint_file.exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}")
    
    print(f"ğŸ“¦ ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {model_info['file']}")
    
    # ê¸°ë³¸ ëª¨ë¸ì˜ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ (text_encoder, vae ë“±)
    base_model_path = model_dir
    dtype = _torch_dtype()
    
    # 1. Text Encoders, Tokenizers (ê¸°ë³¸ ëª¨ë¸ ì¬ì‚¬ìš©)
    print("  [1/3] Text Encoders ë¡œë“œ ì¤‘ (ê¸°ë³¸ ëª¨ë¸ ì¬ì‚¬ìš©)...")
    text_encoder = CLIPTextModel.from_pretrained(
        str(base_model_path),
        subfolder="text_encoder",
        torch_dtype=dtype,
        local_files_only=True,
    )
    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(
        str(base_model_path),
        subfolder="text_encoder_2",
        torch_dtype=dtype,
        local_files_only=True,
    )
    tokenizer = CLIPTokenizer.from_pretrained(
        str(base_model_path),
        subfolder="tokenizer",
        local_files_only=True,
    )
    tokenizer_2 = CLIPTokenizer.from_pretrained(
        str(base_model_path),
        subfolder="tokenizer_2",
        local_files_only=True,
    )
    
    # 2. VAE (ê¸°ë³¸ ëª¨ë¸ ì¬ì‚¬ìš©)
    print("  [2/3] VAE ë¡œë“œ ì¤‘ (ê¸°ë³¸ ëª¨ë¸ ì¬ì‚¬ìš©)...")
    vae = AutoencoderKL.from_pretrained(
        str(base_model_path),
        subfolder="vae",
        torch_dtype=dtype,
        local_files_only=True,
    )
    
    # 3. UNet (ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ë¡œë“œ)
    print("  [3/3] UNet ë¡œë“œ ì¤‘ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)...")
    # ê¸°ë³¸ UNet êµ¬ì¡° ë¡œë“œ
    unet = UNet2DConditionModel.from_pretrained(
        str(base_model_path),
        subfolder="unet",
        torch_dtype=dtype,
        local_files_only=True,
    )
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
    from safetensors import safe_open
    
    # UNet ëª¨ë¸ì˜ ì „ì²´ í‚¤ í™•ì¸
    unet_model_keys = set(unet.state_dict().keys())
    total_unet_keys = len(unet_model_keys)
    print(f"  ğŸ” UNet ëª¨ë¸ ì „ì²´ í‚¤ ê°œìˆ˜: {total_unet_keys}ê°œ")
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
    unet_state_dict = {}
    checkpoint_all_keys = []
    with safe_open(str(checkpoint_file), framework="pt", device="cpu") as f:
        for key in f.keys():
            checkpoint_all_keys.append(key)
            # UNet í‚¤ë§Œ ì¶”ì¶œ
            if key.startswith("model.diffusion_model."):
                unet_key = key.replace("model.diffusion_model.", "")
                unet_state_dict[unet_key] = f.get_tensor(key)
            elif key.startswith("diffusion_model."):
                unet_key = key.replace("diffusion_model.", "")
                unet_state_dict[unet_key] = f.get_tensor(key)
            elif not any(key.startswith(prefix) for prefix in ["first_stage_model.", "cond_stage_model.", "text_encoder", "vae"]):
                # ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ê°€ ì•„ë‹Œ í‚¤ëŠ” UNetìœ¼ë¡œ ê°„ì£¼
                unet_state_dict[key] = f.get_tensor(key)
    
    print(f"  ğŸ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì „ì²´ í‚¤ ê°œìˆ˜: {len(checkpoint_all_keys)}ê°œ")
    print(f"  ğŸ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ì¶œí•œ UNet í‚¤ ê°œìˆ˜: {len(unet_state_dict)}ê°œ")
    
    # ì²´í¬í¬ì¸íŠ¸ í‚¤ ì ‘ë‘ì‚¬ ë¶„ì„
    key_prefixes = {}
    model_keys_sample = []
    for key in checkpoint_all_keys:
        prefix = key.split('.')[0] if '.' in key else key
        if prefix not in key_prefixes:
            key_prefixes[prefix] = 0
        key_prefixes[prefix] += 1
        
        # model. ì ‘ë‘ì‚¬ í‚¤ ìƒ˜í”Œ ìˆ˜ì§‘
        if key.startswith("model.") and len(model_keys_sample) < 10:
            model_keys_sample.append(key)
    
    print(f"  ğŸ” ì²´í¬í¬ì¸íŠ¸ í‚¤ ì ‘ë‘ì‚¬ ë¶„ë¥˜:")
    for prefix, count in sorted(key_prefixes.items()):
        print(f"      {prefix}: {count}ê°œ")
    
    if model_keys_sample:
        print(f"  ğŸ” model. ì ‘ë‘ì‚¬ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
        for key in model_keys_sample:
            print(f"      - {key}")
    
    # UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
    missing_keys, unexpected_keys = unet.load_state_dict(unet_state_dict, strict=False)
    
    # ë§¤ì¹­ í†µê³„
    matched_count = total_unet_keys - len(missing_keys)
    match_rate = (matched_count / total_unet_keys * 100) if total_unet_keys > 0 else 0
    
    print(f"  ğŸ“Š UNet í‚¤ ë§¤ì¹­ í†µê³„:")
    print(f"      ì „ì²´ í‚¤: {total_unet_keys}ê°œ")
    print(f"      ë§¤ì¹­ëœ í‚¤: {matched_count}ê°œ ({match_rate:.1f}%)")
    print(f"      ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ ({100-match_rate:.1f}%)")
    print(f"      ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
    
    if missing_keys:
        if len(missing_keys) > 20:
            print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for key in list(missing_keys)[:10]:
                print(f"      - {key}")
        else:
            print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤:")
            for key in missing_keys:
                print(f"      - {key}")
    
    if unexpected_keys and len(unexpected_keys) <= 20:
        print(f"  âš ï¸  UNet ì˜ˆìƒì¹˜ ëª»í•œ í‚¤:")
        for key in list(unexpected_keys)[:10]:
            print(f"      - {key}")
    
    # ë§¤ì¹­ë¥ ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
    if match_rate < 50:
        print(f"  âš ï¸  ê²½ê³ : UNet í‚¤ ë§¤ì¹­ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ ({match_rate:.1f}%). ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif match_rate < 90:
        print(f"  âš ï¸  ì£¼ì˜: UNet í‚¤ ë§¤ì¹­ë¥ ì´ ì¤‘ê°„ ìˆ˜ì¤€ì…ë‹ˆë‹¤ ({match_rate:.1f}%). ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"  âœ… UNet í‚¤ ë§¤ì¹­ë¥ ì´ ì–‘í˜¸í•©ë‹ˆë‹¤ ({match_rate:.1f}%)")
    
    print("  âœ… UNet ë¡œë“œ ì™„ë£Œ")
    
    # 4. Scheduler
    print("  [4/4] Scheduler ë¡œë“œ ì¤‘...")
    if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            str(base_model_path),
            subfolder="scheduler",
            local_files_only=True,
        )
        scheduler = DPMSolverMultistepScheduler.from_config(
            scheduler.config,
            use_karras=True,
        )
    else:
        scheduler = EulerDiscreteScheduler.from_pretrained(
            str(base_model_path),
            subfolder="scheduler",
            local_files_only=True,
        )
    
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    print("ğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
    pipe = StableDiffusionXLPipeline(
        vae=vae,
        text_encoder=text_encoder,
        text_encoder_2=text_encoder_2,
        tokenizer=tokenizer,
        tokenizer_2=tokenizer_2,
        unet=unet,
        scheduler=scheduler,
    )
    
    # ìµœì í™” ì˜µì…˜ ì ìš©
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸  xFormers í™œì„±í™” ì‹¤íŒ¨: {e}")
        try:
            pipe.enable_attention_slicing(slice_size="auto")
            print("âœ… Attention Slicing í™œì„±í™” (xFormers ëŒ€ì²´)")
        except Exception:
            pass
    
    try:
        pipe.enable_vae_tiling()
        print("âœ… VAE Tiling í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸  VAE Tiling í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    try:
        pipe.enable_vae_slicing()
        print("âœ… VAE Slicing í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸  VAE Slicing í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    # ë””ë°”ì´ìŠ¤ ì´ë™
    if DEVICE == "cuda" and torch.cuda.is_available():
        pipe = pipe.to("cuda")
        print(f"âœ… CUDA ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
    else:
        pipe = pipe.to("cpu")
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    _PIPELINE = pipe
    _PIPELINE_LOADED = True
    print(f"âœ… ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_info['name']}")

def get_refiner_pipeline():
    """
    SDXL Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    í•„ìš”í•  ë•Œë§Œ ë¡œë“œí•˜ê³ , CPU offload ì‚¬ìš©
    """
    global _REFINER_PIPELINE, _REFINER_LOADED
    
    if _REFINER_PIPELINE is not None and _REFINER_LOADED:
        return _REFINER_PIPELINE
    
    if not USE_REFINER:
        return None
    
    print("ğŸ”„ Refiner íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘...")
    dtype = _torch_dtype()
    
    from pathlib import Path
    model_path = Path(MODEL_ID)
    is_local = model_path.exists() and (model_path / "model_index.json").exists()
    
    # Refiner ëª¨ë¸ ê²½ë¡œ í™•ì¸ (ë¡œì»¬ë§Œ ì‚¬ìš©)
    if is_local:
        # ë¡œì»¬ refiner íŒŒì¼ í™•ì¸
        refiner_file = model_path / "sd_xl_refiner_1.0.safetensors"
        if refiner_file.exists():
            print("  ğŸ“ ë¡œì»¬ Refiner íŒŒì¼ ê°ì§€: sd_xl_refiner_1.0.safetensors")
            # ë¡œì»¬ refinerë¥¼ ë¡œë“œí•˜ë ¤ë©´ í‘œì¤€ diffusers í˜•ì‹ì´ í•„ìš”
            # ì¼ë‹¨ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œì—ì„œ ë¡œë“œ ì‹œë„
            refiner_model_id = str(model_path)
            local_files_only = True
        else:
            raise FileNotFoundError(
                f"ë¡œì»¬ Refiner íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}/sd_xl_refiner_1.0.safetensors\n"
                "ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            )
    else:
        raise ValueError("ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ (Img2Img íŒŒì´í”„ë¼ì¸ ì‚¬ìš©, ë¡œì»¬ë§Œ)
    try:
        # ë¡œì»¬ ëª¨ë¸ì—ì„œ Refiner ë¡œë“œ ì‹œë„
        # RefinerëŠ” Img2Img íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¡œë“œë˜ì§€ë§Œ, ë¡œì»¬ì— refiner í´ë”ê°€ ì—†ìœ¼ë©´
        # base ëª¨ë¸ì˜ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¬ì‚¬ìš©í•˜ê³  refiner ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ
        print("  ğŸ”„ ë¡œì»¬ Refiner ë¡œë“œ ì¤‘...")
        
        # ë°©ë²• 1: í‘œì¤€ diffusers í˜•ì‹ refiner í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
        refiner_folder = model_path / "refiner"
        if refiner_folder.exists() and (refiner_folder / "model_index.json").exists():
            refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
                str(refiner_folder),
                torch_dtype=dtype,
                local_files_only=True,
            )
            print("  âœ… í‘œì¤€ diffusers í˜•ì‹ Refiner ë¡œë“œ ì™„ë£Œ")
        # ë°©ë²• 1-2: model_pathì— refiner ì„œë¸Œí´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸ (unet, vae ë“±)
        elif (model_path / "unet").exists() and refiner_file.exists():
            # RefinerëŠ” Baseì™€ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Refiner safetensors íŒŒì¼ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©
            # ì´ ê²½ìš° Refiner UNetì„ ë³„ë„ë¡œ ë¡œë“œí•´ì•¼ í•¨
            print("  ğŸ”„ Refiner UNetì„ ë³„ë„ë¡œ ë¡œë“œ ì¤‘...")
            
            # Base íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°
            base_pipe = get_pipeline()
            
            # Refiner UNetì„ Base UNetê³¼ ë™ì¼í•œ êµ¬ì¡°ë¡œ ìƒì„± (í•˜ì§€ë§Œ Refiner ê°€ì¤‘ì¹˜ ì‚¬ìš©)
            from diffusers import UNet2DConditionModel
            refiner_unet = UNet2DConditionModel.from_pretrained(
                str(model_path),
                subfolder="unet",
                torch_dtype=dtype,
                local_files_only=True,
            )
            
            # Refiner safetensors íŒŒì¼ì—ì„œ UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
            # SDXL Refiner safetensorsëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì§ì ‘ UNet í‚¤ë¥¼ ì‚¬ìš© (ì ‘ë‘ì‚¬ ì—†ìŒ)
            from safetensors import safe_open
            refiner_unet_state_dict = {}
            file_keys = []
            with safe_open(str(refiner_file), framework="pt", device="cpu") as f:
                for key in f.keys():
                    file_keys.append(key)
                    # UNet í‚¤ë§Œ ì¶”ì¶œ (Text Encoder, VAE ì œì™¸)
                    # conditioner.*ëŠ” Text Encoder í‚¤ì´ë¯€ë¡œ ì œì™¸
                    if not any(key.startswith(prefix) for prefix in [
                        "first_stage_model.", "cond_stage_model.", 
                        "text_encoder", "vae", "conditioner"
                    ]):
                        # UNet í‚¤ë¡œ ê°„ì£¼
                        refiner_unet_state_dict[key] = f.get_tensor(key)
            
            # UNet í‚¤ê°€ ì—†ìœ¼ë©´ íŒŒì¼ êµ¬ì¡° í™•ì¸
            if not refiner_unet_state_dict:
                print("  âš ï¸  Refiner safetensors íŒŒì¼ì— UNet í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("  â„¹ï¸  ì´ íŒŒì¼ì€ Text Encoderë§Œ í¬í•¨í•˜ê±°ë‚˜ ë‹¤ë¥¸ í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("  â„¹ï¸  RefinerëŠ” Base UNet ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # ë””ë²„ê¹…: íŒŒì¼ì˜ í‚¤ êµ¬ì¡° í™•ì¸
            print(f"  ğŸ” Refiner íŒŒì¼ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for key in file_keys[:10]:
                print(f"      - {key}")
            print(f"  ğŸ” UNet ëª¨ë¸ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for key in list(refiner_unet.state_dict().keys())[:10]:
                print(f"      - {key}")
            
            # UNet ëª¨ë¸ í‚¤ì™€ íŒŒì¼ í‚¤ ë§¤ì¹­
            refiner_unet_model_keys = set(refiner_unet.state_dict().keys())
            matched_state_dict = {}
            matched_count = 0
            
            # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
            for model_key in refiner_unet_model_keys:
                if model_key in refiner_unet_state_dict:
                    matched_state_dict[model_key] = refiner_unet_state_dict[model_key]
                    matched_count += 1
                # 2. ComfyUI í˜•ì‹ ë³€í™˜ ì‹œë„ (model.diffusion_model. -> )
                elif f"model.diffusion_model.{model_key}" in refiner_unet_state_dict:
                    matched_state_dict[model_key] = refiner_unet_state_dict[f"model.diffusion_model.{model_key}"]
                    matched_count += 1
                # 3. diffusion_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
                elif f"diffusion_model.{model_key}" in refiner_unet_state_dict:
                    matched_state_dict[model_key] = refiner_unet_state_dict[f"diffusion_model.{model_key}"]
                    matched_count += 1
                # 4. model. ì ‘ë‘ì‚¬ ì¶”ê°€ ì‹œë„
                elif f"model.{model_key}" in refiner_unet_state_dict:
                    matched_state_dict[model_key] = refiner_unet_state_dict[f"model.{model_key}"]
                    matched_count += 1
                # 5. unet. ì ‘ë‘ì‚¬ ì¶”ê°€ ì‹œë„
                elif f"unet.{model_key}" in refiner_unet_state_dict:
                    matched_state_dict[model_key] = refiner_unet_state_dict[f"unet.{model_key}"]
                    matched_count += 1
            
            print(f"  ğŸ“Š Refiner UNet í‚¤ ë§¤ì¹­: {matched_count}/{len(refiner_unet_model_keys)}ê°œ")
            
            # UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
            if matched_state_dict:
                missing_keys, unexpected_keys = refiner_unet.load_state_dict(matched_state_dict, strict=False)
                if len(missing_keys) > 100:
                    print(f"  âš ï¸  Refiner UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
                    print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
                elif missing_keys:
                    print(f"  âš ï¸  Refiner UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                if unexpected_keys:
                    print(f"  âš ï¸  Refiner UNet ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                print("  âœ… Refiner UNet ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("  âš ï¸  Refiner UNet ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            
            # Img2Img íŒŒì´í”„ë¼ì¸ ìƒì„± (base ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš©, refiner UNet ì‚¬ìš©)
            refiner = StableDiffusionXLImg2ImgPipeline(
                vae=base_pipe.vae,
                text_encoder=base_pipe.text_encoder,
                text_encoder_2=base_pipe.text_encoder_2,
                tokenizer=base_pipe.tokenizer,
                tokenizer_2=base_pipe.tokenizer_2,
                unet=refiner_unet,  # Refiner UNet ì‚¬ìš©
                scheduler=base_pipe.scheduler,
            )
            refiner = refiner.to(dtype=dtype)
            print("  âœ… Refiner íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
        else:
            # ë°©ë²• 2: base ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš© + refiner safetensors íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            print("  ğŸ“¦ base ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš© + refiner ê°€ì¤‘ì¹˜ ë¡œë“œ...")
            
            # base íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•¨)
            base_pipe = get_pipeline()
            
            # Refiner UNetì„ base UNetê³¼ ë™ì¼í•œ êµ¬ì¡°ë¡œ ìƒì„±
            from diffusers import UNet2DConditionModel
            refiner_unet = UNet2DConditionModel.from_pretrained(
                str(model_path),
                subfolder="unet",
                torch_dtype=dtype,
                local_files_only=True,
            )
            
            # Refiner safetensors íŒŒì¼ì—ì„œ UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
            # UNet ëª¨ë¸ì˜ í‚¤ êµ¬ì¡° í™•ì¸
            refiner_unet_model_keys = set(refiner_unet.state_dict().keys())
            
            # safetensors íŒŒì¼ì—ì„œ ëª¨ë“  í‚¤ ë¡œë“œ
            from safetensors import safe_open
            file_state_dict = {}
            file_keys = []
            with safe_open(str(refiner_file), framework="pt", device="cpu") as f:
                for key in f.keys():
                    file_state_dict[key] = f.get_tensor(key)
                    file_keys.append(key)
            
            # ë””ë²„ê¹…: íŒŒì¼ì˜ í‚¤ êµ¬ì¡° í™•ì¸ (ë°˜ë“œì‹œ ì¶œë ¥)
            print(f"  ğŸ” Refiner íŒŒì¼ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):")
            for i, key in enumerate(file_keys[:20], 1):
                print(f"      {i:2d}. {key}")
            print(f"  ğŸ” UNet ëª¨ë¸ í‚¤ ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):")
            for i, key in enumerate(list(refiner_unet_model_keys)[:20], 1):
                print(f"      {i:2d}. {key}")
            
            # íŒŒì¼ í‚¤ ì ‘ë‘ì‚¬ ë¶„ì„
            file_key_prefixes = {}
            for key in file_keys:
                prefix = key.split('.')[0] if '.' in key else key
                if prefix not in file_key_prefixes:
                    file_key_prefixes[prefix] = 0
                file_key_prefixes[prefix] += 1
            print(f"  ğŸ” íŒŒì¼ í‚¤ ì ‘ë‘ì‚¬ ë¶„ë¥˜:")
            for prefix, count in sorted(file_key_prefixes.items()):
                print(f"      {prefix}: {count}ê°œ")
            
            # UNet ëª¨ë¸ í‚¤ì™€ íŒŒì¼ í‚¤ ë§¤ì¹­
            refiner_unet_state_dict = {}
            matched_count = 0
            matched_patterns = {"direct": 0, "model.diffusion_model": 0, "diffusion_model": 0, "model": 0, "unet": 0}
            
            # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
            for model_key in refiner_unet_model_keys:
                if model_key in file_state_dict:
                    refiner_unet_state_dict[model_key] = file_state_dict[model_key]
                    matched_count += 1
                    matched_patterns["direct"] += 1
                # 2. ComfyUI í˜•ì‹ ë³€í™˜ ì‹œë„ (model.diffusion_model. -> )
                elif f"model.diffusion_model.{model_key}" in file_state_dict:
                    refiner_unet_state_dict[model_key] = file_state_dict[f"model.diffusion_model.{model_key}"]
                    matched_count += 1
                    matched_patterns["model.diffusion_model"] += 1
                # 3. diffusion_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
                elif f"diffusion_model.{model_key}" in file_state_dict:
                    refiner_unet_state_dict[model_key] = file_state_dict[f"diffusion_model.{model_key}"]
                    matched_count += 1
                    matched_patterns["diffusion_model"] += 1
                # 4. model. ì ‘ë‘ì‚¬ ì¶”ê°€ ì‹œë„ (ì¼ë¶€ í˜•ì‹)
                elif f"model.{model_key}" in file_state_dict:
                    refiner_unet_state_dict[model_key] = file_state_dict[f"model.{model_key}"]
                    matched_count += 1
                    matched_patterns["model"] += 1
                # 5. unet. ì ‘ë‘ì‚¬ ì¶”ê°€ ì‹œë„
                elif f"unet.{model_key}" in file_state_dict:
                    refiner_unet_state_dict[model_key] = file_state_dict[f"unet.{model_key}"]
                    matched_count += 1
                    matched_patterns["unet"] += 1
            
            print(f"  ğŸ“Š Refiner UNet í‚¤ ë§¤ì¹­: {matched_count}/{len(refiner_unet_model_keys)}ê°œ")
            if matched_count > 0:
                print(f"  ğŸ“Š ë§¤ì¹­ íŒ¨í„´:")
                for pattern, count in matched_patterns.items():
                    if count > 0:
                        print(f"      {pattern}: {count}ê°œ")
            
            # UNet ê°€ì¤‘ì¹˜ ë¡œë“œ
            if refiner_unet_state_dict:
                missing_keys, unexpected_keys = refiner_unet.load_state_dict(refiner_unet_state_dict, strict=False)
                if len(missing_keys) > 100:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
                    print(f"  âš ï¸  Refiner UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
                    print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
                elif missing_keys:
                    print(f"  âš ï¸  Refiner UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                if unexpected_keys:
                    print(f"  âš ï¸  Refiner UNet ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                print("  âœ… Refiner UNet ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            else:
                print("  âš ï¸  Refiner UNet ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            
            # Img2Img íŒŒì´í”„ë¼ì¸ ìƒì„± (base ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš©, refiner UNet ì‚¬ìš©)
            refiner = StableDiffusionXLImg2ImgPipeline(
                vae=base_pipe.vae,
                text_encoder=base_pipe.text_encoder,
                text_encoder_2=base_pipe.text_encoder_2,
                tokenizer=base_pipe.tokenizer,
                tokenizer_2=base_pipe.tokenizer_2,
                unet=refiner_unet,  # Refiner UNet ì‚¬ìš©
                scheduler=base_pipe.scheduler,
            )
            refiner = refiner.to(dtype=dtype)
        
        # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© (ë¡œì»¬ ëª¨ë¸ì—ì„œ)
        if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
            print("  ğŸ”¥ Refinerì— Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì¤‘...")
            if isinstance(refiner.scheduler, DPMSolverMultistepScheduler):
                refiner.scheduler = DPMSolverMultistepScheduler.from_config(
                    refiner.scheduler.config,
                    use_karras=True,
                )
                print("  âœ… Refiner DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")
            else:
                # ìŠ¤ì¼€ì¤„ëŸ¬ êµì²´
                refiner.scheduler = DPMSolverMultistepScheduler.from_config(
                    base_pipe.scheduler.config if 'base_pipe' in locals() else refiner.scheduler.config,
                    use_karras=True,
                )
                print("  âœ… Refiner DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜
        try:
            refiner.enable_xformers_memory_efficient_attention()
            print("âœ… Refiner: xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
        except Exception:
            refiner.enable_attention_slicing(slice_size="auto")
            print("âœ… Refiner: Attention Slicing í™œì„±í™”")
        
        refiner.enable_vae_tiling()
        refiner.enable_vae_slicing()
        
        # CPU offloadë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ (8GB VRAM ìµœì í™”)
        refiner.enable_model_cpu_offload()
        print("âœ… Refiner: CPU Offload í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
        
        _REFINER_PIPELINE = refiner
        _REFINER_LOADED = True
        print("âœ… Refiner íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
        return _REFINER_PIPELINE
        
    except Exception as e:
        print(f"âŒ Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("  âš ï¸  Refiner ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return None
