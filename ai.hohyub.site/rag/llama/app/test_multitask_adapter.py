"""Test script to verify multitask adapter is properly trained and loaded."""

import os
from pathlib import Path

# Windows multiprocessing ë¬¸ì œ ë°©ì§€
os.environ["HF_DATASETS_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

BASE_DIR = Path(__file__).resolve().parent
BASE_MODEL_PATH = str(BASE_DIR / "model" / "llama_ko")
ADAPTER_PATH = str(BASE_DIR / "model" / "llama_ko_multitask_adapter")
MAX_SEQ_LENGTH = 768

print("=" * 70)
print("ğŸ§ª Multi-Task Adapter ê²€ì¦ í…ŒìŠ¤íŠ¸")
print("=" * 70)
print()

# 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
print("1ï¸âƒ£ íŒŒì¼ ì¡´ì¬ í™•ì¸...")
adapter_files = [
    "adapter_config.json",
    "adapter_model.safetensors",
    "special_tokens_map.json",
    "tokenizer_config.json",
    "tokenizer.json",
]

adapter_dir = Path(ADAPTER_PATH)
missing_files = []
for file in adapter_files:
    file_path = adapter_dir / file
    if file_path.exists():
        size = file_path.stat().st_size / (1024 * 1024)  # MB
        print(f"   âœ… {file}: {size:.2f} MB")
    else:
        print(f"   âŒ {file}: ì—†ìŒ")
        missing_files.append(file)

if missing_files:
    print(f"\nâš ï¸  ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)}")
    print("í›ˆë ¨ì´ ì œëŒ€ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    exit(1)

print("\nâœ… ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤!")

# 2. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
print("\n2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
try:
    from unsloth import FastLanguageModel  # type: ignore

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=ADAPTER_PATH,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
        device_map={"": "cuda:0"},
        max_memory={"cuda:0": "7GiB"},
        low_cpu_mem_usage=True,
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 3. ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (4ê°€ì§€ íƒœìŠ¤í¬)
    print("\n3ï¸âƒ£ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (4ê°€ì§€ íƒœìŠ¤í¬)...")

    FastLanguageModel.for_inference(model)

    test_cases = [
        {
            "name": "ì¹´í…Œê³ ë¦¬ íŒŒì‹±",
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ìƒ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì¡°í™”í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤:\n\n1. **diaries**: ì¼ê¸° í˜•ì‹ì˜ ë‚´ìš©\n2. **accounts**: ì§€ì¶œ ê¸°ë¡ (ê¸ˆì•¡, í•­ëª©, ë‚ ì§œ)\n3. **healthcare_records**: ê±´ê°• ê´€ë ¨ ê¸°ë¡ (í†µì¦, ìš´ë™, ë³‘ì› ë°©ë¬¸ ë“±)\n4. **culture**: ë¬¸í™” í™œë™ (ì˜í™”, ì±…, ì „ì‹œíšŒ ë“±)\n5. **event**: ì¼ì •/ì´ë²¤íŠ¸ (íšŒì˜, ì•½ì†, ì˜ˆì•½ ë“±)\n6. **task**: í•  ì¼ ëª©ë¡\n\nì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ê³ , JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.\nì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ê²½ìš° ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”."
                },
                {"role": "user", "content": "ì˜¤ëŠ˜ ì ì‹¬ì— 8000ì› ì“°ê³  íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì—ˆë‹¤. ë§›ìˆì—ˆë‹¤."}
            ]
        },
        {
            "name": "ê°ì • ë¶„ì„",
            "messages": [
                {
                    "role": "system",
                    "content": "ì¼ê¸° ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°ì§€ëœ ìƒìœ„ 3ê°œ ê°ì •ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. í˜•ì‹: 'ê°ì •1: XX%, ê°ì •2: YY%, ê°ì •3: ZZ%'"
                },
                {"role": "user", "content": "ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤. ì¹œêµ¬ë“¤ê³¼ ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ëƒˆê³  ê¸°ë¶„ì´ ì¢‹ì•˜ë‹¤."}
            ]
        },
        {
            "name": "MBTI ë¶„ì„",
            "messages": [
                {
                    "role": "system",
                    "content": "ì¼ê¸° ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°ì§€ëœ MBTI ì„±ê²© ì°¨ì›ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. 1ê°œë§Œ ê°ì§€ë  ìˆ˜ë„ ìˆê³ , ì—¬ëŸ¬ ê°œê°€ ê°ì§€ë  ìˆ˜ë„ ìˆìœ¼ë©°, ê°ì§€ë˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í˜•ì‹: 'MBTI1: XX%, MBTI2: YY%, MBTI3: ZZ%' ë˜ëŠ” 'í‰ê°€ë¶ˆê°€: 100%'"
                },
                {"role": "user", "content": "ë‚˜ëŠ” ê³„íšì„ ì„¸ìš°ëŠ” ê²ƒì„ ì¢‹ì•„í•œë‹¤. ì—¬í–‰ ê°€ê¸° ì „ì— ëª¨ë“  ì¼ì •ì„ ë¯¸ë¦¬ ì •í•´ë‘”ë‹¤."}
            ]
        },
        {
            "name": "ë¹…íŒŒì´ë¸Œ ë¶„ì„",
            "messages": [
                {
                    "role": "system",
                    "content": "ì¼ê¸° ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°ì§€ëœ ë¹…íŒŒì´ë¸Œ ì„±ê²© íŠ¹ì„±ì„ í¼ì„¼íŠ¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. 1ê°œë§Œ ê°ì§€ë  ìˆ˜ë„ ìˆê³ , ì—¬ëŸ¬ ê°œê°€ ê°ì§€ë  ìˆ˜ë„ ìˆìœ¼ë©°, ê°ì§€ë˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í˜•ì‹: 'íŠ¹ì„±1: XX%, íŠ¹ì„±2: YY%, íŠ¹ì„±3: ZZ%' ë˜ëŠ” 'í‰ê°€ë¶ˆê°€: 100%'"
                },
                {"role": "user", "content": "ë‚˜ëŠ” ìƒˆë¡œìš´ ê²ƒì„ ì‹œë„í•˜ëŠ” ê²ƒì„ ì¢‹ì•„í•œë‹¤. ëª¨í—˜ì ì¸ í™œë™ì„ ì¦ê¸´ë‹¤."}
            ]
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n   í…ŒìŠ¤íŠ¸ {i}: {test_case['name']}")
        print(f"   ì…ë ¥: {test_case['messages'][1]['content'][:50]}...")

        try:
            # Apply chat template
            text = tokenizer.apply_chat_template(
                test_case['messages'],
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = tokenizer([text], return_tensors="pt").to("cuda")

            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extract only the generated part (after the input)
            response_parts = response.split("assistant\n\n")
            if len(response_parts) > 1:
                generated_text = response_parts[-1].split("<|eot_id|>")[0].strip()
            else:
                generated_text = response

            print(f"   ì¶œë ¥: {generated_text[:100]}...")
            print(f"   âœ… ì„±ê³µ")

        except Exception as e:
            print(f"   âŒ ì‹¤íŒ¨: {e}")

    print("\n" + "=" * 70)
    print("âœ… Multi-Task Adapter ê²€ì¦ ì™„ë£Œ!")
    print("=" * 70)
    print(f"ğŸ“ Adapter ìœ„ì¹˜: {ADAPTER_PATH}")
    print("=" * 70)

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

