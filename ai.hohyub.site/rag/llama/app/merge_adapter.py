"""Merge LoRA adapter with base model.

QLoRA í›ˆë ¨ì´ ì™„ë£Œëœ í›„ ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
ë³‘í•©ëœ ëª¨ë¸ì€ Hugging Face Inference APIì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# Add current directory to Python path
sys.path.insert(0, os.path.dirname(__file__))

BASE_DIR = Path(__file__).resolve().parent
BASE_MODEL_PATH = BASE_DIR / "model" / "llama_ko"
ADAPTER_PATH = BASE_DIR / "model" / "llama_ko_adapter"
MERGED_MODEL_PATH = BASE_DIR / "model" / "llama_ko_merged"


def merge_model():
    """Merge LoRA adapter with base model."""
    print("=" * 70)
    print("ğŸ”€ LoRA Adapter ë³‘í•© ì‹œì‘")
    print("=" * 70)
    print(f"ğŸ“Š ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL_PATH}")
    print(f"ğŸ“ ì–´ëŒ‘í„°: {ADAPTER_PATH}")
    print(f"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ: {MERGED_MODEL_PATH}")
    print("=" * 70)
    print()

    # ê²½ë¡œ í™•ì¸
    if not BASE_MODEL_PATH.exists():
        print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {BASE_MODEL_PATH}")
        sys.exit(1)

    if not ADAPTER_PATH.exists():
        print(f"âŒ ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ADAPTER_PATH}")
        sys.exit(1)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    MERGED_MODEL_PATH.mkdir(parents=True, exist_ok=True)

    print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("âš ï¸  ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")

    try:
        from unsloth import FastLanguageModel
        import torch

        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if torch.cuda.is_available():
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        else:
            print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUì—ì„œ ë³‘í•©í•˜ë©´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")

        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ë³‘í•©ì„ ìœ„í•´ 4bit ë¹„í™œì„±í™”)
        print("\n1ï¸âƒ£ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=str(BASE_MODEL_PATH),
            max_seq_length=768,  # ì›ë˜ í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ê°’
            dtype=None,
            load_in_4bit=False,  # ë³‘í•©ì„ ìœ„í•´ 4bit ë¹„í™œì„±í™”
            device_map="auto",
        )
        print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

        # ì–´ëŒ‘í„° ë¡œë“œ
        print("\n2ï¸âƒ£ ì–´ëŒ‘í„° ë¡œë”©...")
        model.load_adapter(str(ADAPTER_PATH))
        print("âœ… ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ!")

        # ì–´ëŒ‘í„° ë³‘í•©
        print("\n3ï¸âƒ£ ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
        print("âš ï¸  ì´ ê³¼ì •ì€ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        model = model.merge_and_unload()  # ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
        print("âœ… ì–´ëŒ‘í„° ë³‘í•© ì™„ë£Œ!")

        # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
        print("\n4ï¸âƒ£ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
        print("âš ï¸  ëŒ€ìš©ëŸ‰ íŒŒì¼ ì €ì¥ìœ¼ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        model.save_pretrained(str(MERGED_MODEL_PATH), safe_serialization=True)
        tokenizer.save_pretrained(str(MERGED_MODEL_PATH))
        print("âœ… ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 70)
        print("ğŸ‰ ë³‘í•© ì™„ë£Œ!")
        print("=" * 70)
        print(f"ğŸ“ ë³‘í•©ëœ ëª¨ë¸ ìœ„ì¹˜: {MERGED_MODEL_PATH}/")
        print()
        print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. Hugging Faceì— ì—…ë¡œë“œ:")
        print(f"   huggingface-cli upload your-username/model-name {MERGED_MODEL_PATH}/")
        print()
        print("2. ë˜ëŠ” Pythonìœ¼ë¡œ ì—…ë¡œë“œ:")
        print("   from huggingface_hub import HfApi")
        print(f"   api = HfApi()")
        print(f"   api.upload_folder(")
        print(f"       folder_path='{MERGED_MODEL_PATH}',")
        print(f"       repo_id='your-username/model-name',")
        print(f"       repo_type='model',")
        print(f"   )")
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    merge_model()

