"""
ğŸ˜ğŸ˜ chat_service.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

ë‹¨ìˆœ ì±„íŒ…/ëŒ€í™”í˜• LLM ì¸í„°í˜ì´ìŠ¤.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.
"""

import os
from typing import Any, List, Optional

import torch
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain_huggingface import HuggingFacePipeline
from langchain_postgres import PGVector
from peft import (  # type: ignore
    LoraConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    PreTrainedTokenizerFast,
    pipeline,
)



def _get_model_path() -> str:
    """Get model path from S3 or local location.

    Returns:
        Normalized absolute path to the model directory.

    Raises:
        FileNotFoundError: If model directory is not found.
    """
    # S3ì—ì„œ ëª¨ë¸ ë¡œë“œ (ìš°ì„ )
    s3_bucket = os.getenv("S3_MODEL_BUCKET")
    if s3_bucket:
        try:
            from utils.s3_model_loader import load_model_directory_from_s3
            
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì´ë¦„ (ê¸°ë³¸ê°’: llama_ko)
            model_dir_name = os.getenv("S3_MODEL_DIR_NAME", "llama_ko")
            
            print(f"ğŸ“¦ S3ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œë„: {s3_bucket}/{model_dir_name}")
            model_path = load_model_directory_from_s3(
                model_dir_name=model_dir_name,
                bucket_name=s3_bucket,
            )
            print(f"âœ… S3ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            return model_path
        except Exception as e:
            print(f"âš ï¸  S3ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë¡œ í´ë°±í•©ë‹ˆë‹¤...")
            # S3 ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ê²½ë¡œë¡œ í´ë°±
    
    # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (í´ë°±)
    local_model_dir = os.getenv("LOCAL_MODEL_DIR")

    if local_model_dir:
        # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        if os.path.isabs(local_model_dir):
            model_path = local_model_dir
        else:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° langchain ë£¨íŠ¸ í´ë” ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            model_path = os.path.join(root_dir, local_model_dir.lstrip("./"))
    else:
        # ê¸°ë³¸ê°’: app/model/llama_ko (app í´ë” ê¸°ì¤€)
        app_dir = os.path.dirname(os.path.dirname(__file__))
        model_path = os.path.join(app_dir, "model", "llama_ko")

    # ê²½ë¡œ ì •ê·œí™”
    model_path = os.path.normpath(os.path.abspath(model_path))

    # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        raise FileNotFoundError(
            f"Model directory not found: {model_path}\n"
            f"Please set LOCAL_MODEL_DIR in .env file or ensure model exists.\n"
            f"Or set S3_MODEL_BUCKET to load from S3."
        )

    return model_path


def train_qlora_adapter(
    model_path: str,
    output_dir: str = "app/model/llama_ko_adapter",
    *,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
) -> str:
    """Train QLoRA adapter for fine-tuning.

    Args:
        model_path: Path to base model.
        output_dir: Directory to save adapter.
        lora_r: LoRA rank (lower = fewer parameters, faster training).
        lora_alpha: LoRA alpha scaling factor.
        lora_dropout: LoRA dropout rate.

    Returns:
        Path to saved adapter directory.

    Raises:
        FileNotFoundError: If model directory is not found.
    """
    print("\n" + "=" * 60)
    print("Starting QLoRA adapter training...")
    print("=" * 60)

    # 4bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # ëª¨ë¸ ë¡œë“œ
    print("Loading base model...")
    model: Any = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        dtype=torch.bfloat16,
    )

    # ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„
    print("Preparing model for k-bit training...")
    model = prepare_model_for_kbit_training(model)

    # LoRA ì„¤ì •
    print(
        f"Configuring LoRA (r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout})..."
    )
    peft_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Llama attention
    )

    # PEFT ëª¨ë¸ ìƒì„±
    print("Creating PEFT model...")
    model = get_peft_model(model, peft_config)  # type: ignore
    model.print_trainable_parameters()  # type: ignore

    # ì–´ëŒ‘í„° ì €ì¥
    print(f"\nSaving adapter to: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    print("=" * 60)
    print("âœ… QLoRA adapter created successfully!")
    print(f"âœ… Adapter saved to: {output_dir}")
    print("\nTo use the adapter, set in .env file:")
    print(f"PEFT_ADAPTER_PATH={output_dir}")
    print("=" * 60 + "\n")

    return output_dir


def init_tokenizer() -> PreTrainedTokenizerFast:
    """Initialize tokenizer for Llama-3.1-Korean-8B-Instruct model.

    Returns:
        PreTrainedTokenizerFast instance with Llama-3.1 tokenizer.

    Raises:
        FileNotFoundError: If model directory is not found.
    """
    print("[CHAT_SERVICE] ğŸ”¤ init_tokenizer() called")
    model_path = _get_model_path()

    print("Loading Llama-3.1 tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

    # Llama-3.1 í† í¬ë‚˜ì´ì €ëŠ” EOSë¥¼ PADë¡œ ì‚¬ìš©
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"Tokenizer vocab size: {len(tokenizer)}")
    print(f"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    print(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")

    return tokenizer


def init_llm() -> Any:  # type: ignore
    """Initialize Llama LLM.

    Returns:
        HuggingFacePipeline instance.

    Raises:
        FileNotFoundError: If model directory is not found.
    """
    print("\n" + "=" * 60)
    print("[CHAT_SERVICE] ğŸš€ init_llm() called - Starting Llama LLM initialization")
    print("=" * 60)

    # Llama provider
    model_path = _get_model_path()
    print(f"Loading local model from: {model_path}")
    print("Using 4bit quantization...")

    # GPU í™•ì¸
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        print(f"GPU detected: {torch.cuda.get_device_name(0)}")
        print(
            f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB"
        )
        print("Using 4bit quantization on GPU...")
    else:
        print("WARNING: No GPU detected. Using CPU mode (will be VERY slow)")
        print("CPU mode: Using float32 without quantization for compatibility")

    # í† í¬ë‚˜ì´ì € ë¡œë”© (ë³„ë„ í•¨ìˆ˜ ì‚¬ìš©)
    tokenizer = init_tokenizer()

    # GPU/CPUì— ë”°ë¼ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš©
    model: Any
    if use_gpu:
        # GPU: 4bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ 11GB -> 3.5GB, ì†ë„ 2-4ë°° í–¥ìƒ)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        print("Loading model with 4bit quantization on GPU (this may take a while)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",  # Automatically use GPU
            dtype=torch.bfloat16,
        )
        print("[OK] Base model loaded with 4bit quantization on GPU")
    else:
        # CPU: ì–‘ìí™” ì—†ì´ float32ë¡œ ë¡œë“œ (CPUëŠ” ì–‘ìí™” ì§€ì› ì œí•œì )
        print("Loading model on CPU without quantization (this may take a while)...")
        print("Note: CPU inference will be very slow. Consider using GPU if available.")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="cpu",  # Explicitly use CPU
            torch_dtype=torch.float32,  # CPUëŠ” float32 ì‚¬ìš©
            low_cpu_mem_usage=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
        )
        print("[OK] Base model loaded on CPU (no quantization)")

    # PEFT/QLoRA adapter ë¡œë“œ (ì„ íƒì )
    print("\n" + "=" * 60)
    print("[QLoRA] Checking for PEFT/QLoRA adapter...")
    print("=" * 60)
    peft_adapter_path = os.getenv("PEFT_ADAPTER_PATH")
    peft_loaded = False

    if peft_adapter_path:
        # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        if os.path.isabs(peft_adapter_path):
            adapter_path = peft_adapter_path
        else:
            root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            adapter_path = os.path.join(root_dir, peft_adapter_path.lstrip("./"))

        adapter_path = os.path.normpath(os.path.abspath(adapter_path))

        print(f"[QLoRA] Checking adapter path: {adapter_path}")

        if os.path.exists(adapter_path):
            print(f"[QLoRA] Loading PEFT/QLoRA adapter from: {adapter_path}")
            try:
                # 4bit ì–‘ìí™”ëœ ëª¨ë¸ì— PEFT adapter ë¡œë“œ
                model = PeftModel.from_pretrained(
                    model,
                    adapter_path,
                    device_map="auto",  # GPU ìë™ í• ë‹¹
                )
                peft_loaded = True
                print("[QLoRA] âœ… PEFT/QLoRA adapter loaded successfully!")
                print(f"[QLoRA] Model type: {type(model).__name__}")
            except Exception as e:
                print(f"[QLoRA] âŒ ERROR: Failed to load PEFT adapter: {e}")
                import traceback

                traceback.print_exc()
                print("[QLoRA] Continuing with base model...")
        else:
            print(f"[QLoRA] âš ï¸  WARNING: PEFT adapter path not found: {adapter_path}")
    else:
        print("[QLoRA] â„¹ï¸  No PEFT_ADAPTER_PATH specified in environment variables")

    # ìµœì¢… ìƒíƒœ ì¶œë ¥
    print("=" * 60)
    if peft_loaded:
        print("[QLoRA] âœ… Status: QLoRA adapter is ACTIVE")
        print("[QLoRA] âœ… Fine-tuned model is ready!")
    else:
        print("[QLoRA] â„¹ï¸  Status: Using base model (no QLoRA adapter)")
        print("[QLoRA] â„¹ï¸  To use QLoRA, set PEFT_ADAPTER_PATH in .env file")
    print("=" * 60 + "\n")

    print("Creating pipeline with Llama-3.1 optimized settings...")
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„± (Llama-3.1 ì¶”ë¡ í˜• ëª¨ë¸ ìµœì í™”)
    pipe = pipeline(
        "text-generation",
        model=model,  # type: ignore
        tokenizer=tokenizer,
        max_new_tokens=200,  # ì¶”ë¡  ê³¼ì •ì„ ìœ„í•œ ì¶©ë¶„í•œ ê¸¸ì´
        do_sample=True,  # ìƒ˜í”Œë§ìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
        temperature=0.6,  # ì¶”ë¡ í˜• ëª¨ë¸ì´ë¯€ë¡œ ì•½ê°„ ë‚®ì¶¤ (ë” ì¼ê´€ì„± ìˆê²Œ)
        top_p=0.9,  # Nucleus sampling
        top_k=50,  # Top-k sampling ì¶”ê°€ (Llama-3 ê¶Œì¥)
        repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€ (ë„ˆë¬´ ë†’ìœ¼ë©´ ì¶”ë¡ ì´ ëŠê¹€)
        return_full_text=False,  # ì…ë ¥ í…ìŠ¤íŠ¸ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        pad_token_id=tokenizer.pad_token_id,  # íŒ¨ë”© í† í° ì„¤ì •
        eos_token_id=tokenizer.eos_token_id,  # EOS í† í° ì„¤ì •
    )

    # LangChain LLM ê°ì²´ë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    print("[OK] Llama-3.1-Korean-8B-Instruct LLM initialized with 4bit quantization!")
    print("[CHAT_SERVICE] âœ… init_llm() completed - Returning HuggingFacePipeline")
    print("=" * 60 + "\n")
    return llm


def _init_llm_legacy() -> Any:  # type: ignore
    """Legacy LLM initialization (kept for reference)."""
    # Llama provider (default)
    model_path = _get_model_path()
    print(f"Loading local model from: {model_path}")
    print("Using 4bit quantization...")

    # GPU í™•ì¸
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        print(f"GPU detected: {torch.cuda.get_device_name(0)}")
        print(
            f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB"
        )
        print("Using 4bit quantization on GPU...")
    else:
        print("WARNING: No GPU detected. Using CPU mode (will be VERY slow)")
        print("CPU mode: Using float32 without quantization for compatibility")

    # í† í¬ë‚˜ì´ì € ë¡œë”© (ë³„ë„ í•¨ìˆ˜ ì‚¬ìš©)
    tokenizer = init_tokenizer()

    # GPU/CPUì— ë”°ë¼ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš©
    model: Any
    if use_gpu:
        # GPU: 4bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ 11GB -> 3.5GB, ì†ë„ 2-4ë°° í–¥ìƒ)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        print("Loading model with 4bit quantization on GPU (this may take a while)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",  # Automatically use GPU
            dtype=torch.bfloat16,
        )
        print("[OK] Base model loaded with 4bit quantization on GPU")
    else:
        # CPU: ì–‘ìí™” ì—†ì´ float32ë¡œ ë¡œë“œ (CPUëŠ” ì–‘ìí™” ì§€ì› ì œí•œì )
        print("Loading model on CPU without quantization (this may take a while)...")
        print("Note: CPU inference will be very slow. Consider using GPU if available.")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="cpu",  # Explicitly use CPU
            torch_dtype=torch.float32,  # CPUëŠ” float32 ì‚¬ìš©
            low_cpu_mem_usage=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
        )
        print("[OK] Base model loaded on CPU (no quantization)")

    # PEFT/QLoRA adapter ë¡œë“œ (ì„ íƒì )
    print("\n" + "=" * 60)
    print("[QLoRA] Checking for PEFT/QLoRA adapter...")
    print("=" * 60)
    peft_adapter_path = os.getenv("PEFT_ADAPTER_PATH")
    peft_loaded = False

    if peft_adapter_path:
        # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        if os.path.isabs(peft_adapter_path):
            adapter_path = peft_adapter_path
        else:
            root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            adapter_path = os.path.join(root_dir, peft_adapter_path.lstrip("./"))

        adapter_path = os.path.normpath(os.path.abspath(adapter_path))

        print(f"[QLoRA] Checking adapter path: {adapter_path}")

        if os.path.exists(adapter_path):
            print(f"[QLoRA] Loading PEFT/QLoRA adapter from: {adapter_path}")
            try:
                # 4bit ì–‘ìí™”ëœ ëª¨ë¸ì— PEFT adapter ë¡œë“œ
                # adapter_name_or_pathë¥¼ í‚¤ì›Œë“œ ì¸ìë¡œ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                model = PeftModel.from_pretrained(
                    model,
                    adapter_path,
                    device_map="auto",  # GPU ìë™ í• ë‹¹
                )
                peft_loaded = True
                print("[QLoRA] âœ… PEFT/QLoRA adapter loaded successfully!")
                print(f"[QLoRA] Model type: {type(model).__name__}")
            except Exception as e:
                print(f"[QLoRA] âŒ ERROR: Failed to load PEFT adapter: {e}")
                import traceback

                traceback.print_exc()
                print("[QLoRA] Continuing with base model...")
        else:
            print(f"[QLoRA] âš ï¸  WARNING: PEFT adapter path not found: {adapter_path}")
    else:
        print("[QLoRA] â„¹ï¸  No PEFT_ADAPTER_PATH specified in environment variables")

    # ìµœì¢… ìƒíƒœ ì¶œë ¥
    print("=" * 60)
    if peft_loaded:
        print("[QLoRA] âœ… Status: QLoRA adapter is ACTIVE")
        print("[QLoRA] âœ… Fine-tuned model is ready!")
    else:
        print("[QLoRA] â„¹ï¸  Status: Using base model (no QLoRA adapter)")
        print("[QLoRA] â„¹ï¸  To use QLoRA, set PEFT_ADAPTER_PATH in .env file")
    print("=" * 60 + "\n")

    print("Creating pipeline with Llama-3.1 optimized settings...")
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„± (Llama-3.1 ì¶”ë¡ í˜• ëª¨ë¸ ìµœì í™”)
    # PeftModelì€ PreTrainedModelì„ ë˜í•‘í•˜ë¯€ë¡œ pipelineì— ì „ë‹¬ ê°€ëŠ¥
    pipe = pipeline(
        "text-generation",
        model=model,  # type: ignore
        tokenizer=tokenizer,
        max_new_tokens=200,  # ì¶”ë¡  ê³¼ì •ì„ ìœ„í•œ ì¶©ë¶„í•œ ê¸¸ì´
        do_sample=True,  # ìƒ˜í”Œë§ìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
        temperature=0.6,  # ì¶”ë¡ í˜• ëª¨ë¸ì´ë¯€ë¡œ ì•½ê°„ ë‚®ì¶¤ (ë” ì¼ê´€ì„± ìˆê²Œ)
        top_p=0.9,  # Nucleus sampling
        top_k=50,  # Top-k sampling ì¶”ê°€ (Llama-3 ê¶Œì¥)
        repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€ (ë„ˆë¬´ ë†’ìœ¼ë©´ ì¶”ë¡ ì´ ëŠê¹€)
        return_full_text=False,  # ì…ë ¥ í…ìŠ¤íŠ¸ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        pad_token_id=tokenizer.pad_token_id,  # íŒ¨ë”© í† í° ì„¤ì •
        eos_token_id=tokenizer.eos_token_id,  # EOS í† í° ì„¤ì •
    )

    # LangChain LLM ê°ì²´ë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    print("[OK] Llama-3.1-Korean-8B-Instruct LLM initialized with 4bit quantization!")
    print("[CHAT_SERVICE] âœ… init_llm() completed - Returning HuggingFacePipeline")
    print("=" * 60 + "\n")
    return llm


def create_rag_chain(vector_store: PGVector, llm: Any) -> Runnable:  # type: ignore
    """Create RAG chain with retriever and LLM.

    Args:
        vector_store: PGVector instance for document retrieval.
        llm: HuggingFacePipeline instance for generation.

    Returns:
        RAG chain (runnable).
    """
    print("\n" + "=" * 60)
    print("[CHAT_SERVICE] ğŸ”— create_rag_chain() called")
    print(f"[CHAT_SERVICE]   - vector_store type: {type(vector_store).__name__}")
    print(f"[CHAT_SERVICE]   - llm type: {type(llm).__name__}")
    print("=" * 60)

    # Llama-3.1-Korean-Reasoningìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    # ì´ ëª¨ë¸ì€ ì¶”ë¡ í˜• ëª¨ë¸ì´ë¯€ë¡œ ë‹¨ê³„ì  ì‚¬ê³ ë¥¼ ìœ ë„í•˜ëŠ” instruction í˜•ì‹ ì‚¬ìš©
    template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹¨ê³„ì ìœ¼ë¡œ ì‚¬ê³ í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ì°¸ê³  ì •ë³´ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì°¸ê³  ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. ì¸ì‚¬ë§ì—ëŠ” ê°„ë‹¨íˆ ì¸ì‚¬ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”
4. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
5. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”<|eot_id|><|start_header_id|>user<|end_header_id|>

{history}

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

    prompt = PromptTemplate.from_template(template)

    def format_docs(docs: List[Document]) -> str:
        return "\n\n".join(doc.page_content for doc in docs)

    def format_history(history: Optional[List[dict]]) -> str:
        """Format conversation history for the prompt."""
        if not history or len(history) == 0:
            return ""

        # ìµœê·¼ 10ê°œ ëŒ€í™”ë§Œ í¬í•¨ (í† í° ì œí•œ ê³ ë ¤)
        recent_history = history[-10:] if len(history) > 10 else history

        history_text = "ì´ì „ ëŒ€í™”:\n"
        for msg in recent_history:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                history_text += f"ì‚¬ìš©ì: {content}\n"
            elif role == "assistant":
                history_text += f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}\n"

        return history_text + "\n"

    def create_rag_input(input_data: dict) -> dict:
        """Create input for RAG chain with history.

        Note: Retriever is called separately in the router to support async_mode.
        """
        print("[CHAT_SERVICE] ğŸ“¥ create_rag_input() called")
        print(f"[CHAT_SERVICE]   - question: {input_data.get('question', '')[:50]}...")
        print(
            f"[CHAT_SERVICE]   - history length: {len(input_data.get('history', []))}"
        )
        print(
            f"[CHAT_SERVICE]   - context length: {len(input_data.get('context', ''))}"
        )
        question = input_data.get("question", "")
        history = input_data.get("history", None)

        # Documents will be retrieved separately in the router
        # This function just formats the input
        return {
            "context": input_data.get("context", ""),
            "history": format_history(history),
            "question": question,
        }

    rag_chain: Runnable = create_rag_input | prompt | llm | StrOutputParser()

    print("[CHAT_SERVICE] âœ… create_rag_chain() completed - Returning RAG chain")
    print("=" * 60 + "\n")
    return rag_chain
