"""ÌÜµÌï© RAG ÏÑúÎπÑÏä§ FastAPI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò
OpenAIÏôÄ Llama RAG ÏÑúÎπÑÏä§Î•º ÌïòÎÇòÏùò Ìè¨Ìä∏Î°ú ÌÜµÌï©
"""

import asyncio
import os
import sys
from contextlib import asynccontextmanager
from pathlib import Path

# Fix for Windows: psycopg requires SelectorEventLoop, not ProactorEventLoop
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º Python pathÏóê Ï∂îÍ∞Ä
# main.pyÍ∞Ä rag/ Ìè¥ÎçîÏóê ÏßÅÏ†ë ÏûàÏúºÎØÄÎ°ú parentÍ∞Ä rag Ìè¥Îçî
project_root = Path(__file__).parent.absolute()
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"‚úÖ Loaded .env from: {env_path}")
else:
    print(f"‚ö†Ô∏è  No .env file found at: {env_path}")

# ÏÑúÎπÑÏä§ Í∞ÄÏö©ÏÑ± ÌîåÎûòÍ∑∏
OPENAI_AVAILABLE = False
LLAMA_AVAILABLE = False

# RAG OpenAI ÎùºÏö∞ÌÑ∞ import
try:
    # app/main.py -> rag/ -> ai.hohyub.site/rag/openai/app
    openai_app_path = project_root / "openai" / "app"
    sys.path.insert(0, str(openai_app_path))
    from router import chat_router as openai_chat_router, search_router as openai_search_router
    from service.chat_service import create_rag_chain as create_openai_rag_chain, init_llm as init_openai_llm
    from service.vectorstore import init_vector_store as init_openai_vector_store
    OPENAI_AVAILABLE = True
    print("‚úÖ OpenAI RAG Î™®Îìà Î°úÎìú ÏôÑÎ£å")
except Exception as e:
    print(f"‚ö†Ô∏è  OpenAI RAG ÏÑúÎπÑÏä§ Î°úÎìú Ïã§Ìå®: {e}")
    import traceback
    traceback.print_exc()
    openai_chat_router = None
    openai_search_router = None
    init_openai_vector_store = None
    create_openai_rag_chain = None
    init_openai_llm = None

# RAG Llama ÎùºÏö∞ÌÑ∞ import
try:
    # app/main.py -> rag/ -> ai.hohyub.site/rag/llama/app
    llama_app_path = project_root / "llama" / "app"
    sys.path.insert(0, str(llama_app_path))
    from router import chat_router as llama_chat_router
    from service.chat_service import create_rag_chain as create_llama_rag_chain, init_llm as init_llama_llm
    # Llama vectorstore Ï¥àÍ∏∞Ìôî Ìï®Ïàò Ï∞æÍ∏∞
    try:
        from core.vectorstore import init_vector_store as init_llama_vector_store
    except:
        # core.vectorstoreÍ∞Ä ÏóÜÏúºÎ©¥ ÏßÅÏ†ë PGVector ÏÉùÏÑ±
        init_llama_vector_store = None
    LLAMA_AVAILABLE = True
    print("‚úÖ Llama RAG Î™®Îìà Î°úÎìú ÏôÑÎ£å")
except Exception as e:
    print(f"‚ö†Ô∏è  Llama RAG ÏÑúÎπÑÏä§ Î°úÎìú Ïã§Ìå®: {e}")
    import traceback
    traceback.print_exc()
    llama_chat_router = None
    create_llama_rag_chain = None
    init_llama_llm = None
    init_llama_vector_store = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown."""
    # Startup
    try:
        # OpenAI RAG Ï¥àÍ∏∞Ìôî
        if OPENAI_AVAILABLE and init_openai_vector_store:
            print("Initializing OpenAI RAG...")
            try:
                openai_vector_store = await init_openai_vector_store()
                openai_llm = init_openai_llm()
                openai_rag_chain = create_openai_rag_chain(openai_vector_store, openai_llm)
                if openai_chat_router:
                    openai_chat_router.set_dependencies(openai_vector_store, openai_rag_chain)
                if openai_search_router:
                    openai_search_router.set_dependencies(openai_vector_store)
                print("[OK] OpenAI RAG initialized!")
            except Exception as e:
                print(f"[ERROR] OpenAI RAG initialization failed: {e}")
                import traceback
                traceback.print_exc()
        
        # Llama RAG Ï¥àÍ∏∞Ìôî
        if LLAMA_AVAILABLE and init_llama_vector_store:
            print("Initializing Llama RAG...")
            try:
                llama_vector_store = await init_llama_vector_store()
                llama_llm = init_llama_llm()
                llama_rag_chain = create_llama_rag_chain(llama_vector_store, llama_llm)
                if llama_chat_router:
                    llama_chat_router.set_dependencies(llama_vector_store, llama_rag_chain)
                print("[OK] Llama RAG initialized!")
            except Exception as e:
                print(f"[ERROR] Llama RAG initialization failed: {e}")
                import traceback
                traceback.print_exc()
        
        print("‚úÖ All RAG services initialized!")
    except Exception as e:
        print(f"[ERROR] Startup error: {e}")
        import traceback
        traceback.print_exc()
        raise

    yield

    # Shutdown
    print("Shutting down...")


app = FastAPI(
    title="RAG Service Platform",
    description="ÌÜµÌï© RAG ÏÑúÎπÑÏä§ ÌîåÎû´Ìèº - OpenAI Î∞è Llama",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ÏÑ§Ï†ï
allowed_origins_str = os.getenv(
    "ALLOWED_ORIGINS",
    "http://localhost:3000,http://localhost:5000,http://localhost:7000,http://127.0.0.1:3000,http://127.0.0.1:5000,http://127.0.0.1:7000"
)
if allowed_origins_str == "*":
    allowed_origins = ["*"]
else:
    allowed_origins = [origin.strip() for origin in allowed_origins_str.split(",")]

print(f"üåê CORS allowed origins: {allowed_origins}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# RAG OpenAI ÎùºÏö∞ÌÑ∞ Ìè¨Ìï®
if OPENAI_AVAILABLE and openai_chat_router:
    app.include_router(openai_chat_router.router, prefix="/rag/openai", tags=["RAG OpenAI"])
if OPENAI_AVAILABLE and openai_search_router:
    app.include_router(openai_search_router.router, prefix="/rag/openai", tags=["RAG OpenAI"])

# RAG Llama ÎùºÏö∞ÌÑ∞ Ìè¨Ìï®
if LLAMA_AVAILABLE and llama_chat_router:
    app.include_router(llama_chat_router.router, prefix="/rag/llama", tags=["RAG Llama"])


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "RAG Service Platform",
        "version": "1.0.0",
        "services": {
            "rag_openai": "available" if OPENAI_AVAILABLE else "unavailable",
            "rag_llama": "available" if LLAMA_AVAILABLE else "unavailable",
        },
        "endpoints": {
            "rag_openai": {
                "rag": "POST /rag/openai/rag",
                "retrieve": "POST /rag/openai/retrieve",
            },
            "rag_llama": {
                "rag": "POST /rag/llama/rag",
            },
            "health": "GET /health",
        },
    }


@app.get("/health")
async def health():
    """ÌÜµÌï© Ìó¨Ïä§Ï≤¥ÌÅ¨"""
    return {
        "status": "healthy",
        "services": {
            "rag_openai": "initialized" if OPENAI_AVAILABLE else "unavailable",
            "rag_llama": "initialized" if LLAMA_AVAILABLE else "unavailable",
        }
    }


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)

